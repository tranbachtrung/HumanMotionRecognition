{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7.1 & 7.2 - Gateaux and Frechet Differential.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbt004/HumanMotionRecognition/blob/master/7_1_%26_7_2_Gateaux_and_Frechet_Differential.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1ewlRwiGIj4"
      },
      "source": [
        "# Optimization of Functionals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVWFJ7Cf-maI"
      },
      "source": [
        "## 7.1 Introduction\n",
        "\n",
        "Two representation of functionals. Graph representation (fig 7.1) and hyperplane (fig 7.2). Regarding optimization, the first half of this section deals with the hyperplane representation, while the second half deals with the graph representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNa0JumQ_l0I"
      },
      "source": [
        "## 7.2 Gateaux and Frechet Differentials\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMwXMuze_mOW"
      },
      "source": [
        "### Gateaux Differential \n",
        "\n",
        "In this section, let $X$ be a vector space, $Y$ a normed space and $T$ a transformation defined on a domain $D \\subseteq X$ and having range $R \\subseteq Y$.\n",
        "\n",
        "**Definition**: Let $x \\in D \\subseteq X$ and let $h$ be arbitrary in $X$. If the limit\n",
        "\\begin{equation}\n",
        "\\delta T(x; h) = \\lim_{\\alpha \\to 0} \\frac{1}{\\alpha} [T(x + \\alpha h) -  T(x)]\n",
        "\\end{equation}\n",
        "exists, it is called the **Gateaux differential** of $T$ at $x$ with increment $h$. If the limit of the above equation exists for each $h \\in X$, the transformation $T$ is said to be **Gateaux differentiable** at $x$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7jLLuaLMIgs"
      },
      "source": [
        "\n",
        "> *Note*: To understand this, let us remind ourselves the definition of a functional limit for metric spaces.\n",
        "\n",
        "> **Definition:** Let $f: X \\to Y$ and $(X, d_X)$ and $(Y, d_Y)$ be metric spaces. We say that\n",
        "$$\n",
        "\\lim_{x \\to x_0} f(x) = y_0\n",
        "$$\n",
        "if $\\forall \\varepsilon > 0$, there exists a real $\\delta > 0$ such that for all $x \\in X$ such that $d_X(x, x_0) <\\delta$ implies that $d(f(x), y_0) < \\varepsilon$. Note that $x_0$ need not be in the domain of $f$, nor does $y_0$ need to be in the range of $f$, and even if $f(x_0)$ is defined it need not be equal to $y_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLPRoZyoORdk"
      },
      "source": [
        "> *Note*: For the Gateaux differential definition, if we fix $x$ and $h$ and define $T': \\mathbb{R} \\to Y$ by $T'(\\alpha) = \\frac{1}{\\alpha} [T(x + \\alpha h) -  T(x)]$, then we can use the definition from above. This means that there exist $y_0 \\in Y$ such that\n",
        "\\begin{equation}\n",
        "\\lim_{\\alpha \\to 0} T'(\\alpha) = y_0.\n",
        "\\end{equation}\n",
        "This means for all $\\varepsilon>0$ , there exists a real  $\\delta>0$  such that for all real  $\\alpha$  such that  $|\\alpha|<δ$  implies that  $\\lVert T'(\\alpha) - y_0\\rVert_Y<\\varepsilon$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGe5a2_eIUWQ"
      },
      "source": [
        "We note that it makes sense to consider the above limit only if $x + {\\alpha}h \\in D$ for all $\\alpha$ sufficiently small. The limit is, of course, taken in the usual sense of norm convergences in $Y$. We observe that for fixed $x\\in D$ and $h$ regarded as variable, the Gateaux differential defines a transformation from $X$ to $Y$. In the particular case where $T$ is linear, we have $\\delta T(x; h) = T(h)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCuiVCvVCb5t"
      },
      "source": [
        "If $f$ is a real functional on $X$, the Gateaux differential of $f$, if it exists, is\n",
        "$$\n",
        "\\delta f(x; h) = \\frac{d}{d\\alpha} f(x + \\alpha h) \\bigg\\rvert_{\\alpha = 0}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS4J8AfxSPl_"
      },
      "source": [
        ">*Note:* In this case, fixing $x$ and $h$ in $X$, define $f': \\mathbb{R} \\to \\mathbb{R}$ by $f'(\\alpha) = f(x + \\alpha h)$. Therefore, by definition of derivative, we obtain:\n",
        "$$\n",
        "\\frac{d}{d\\alpha} f'(\\alpha) \\bigg\\lvert_{\\alpha = 0}  = \\lim_{\\alpha \\to 0} \\frac{f'(\\alpha) - f'(0)}{\\alpha - 0} = \\lim_{\\alpha \\to 0} \\frac{1}{\\alpha} [f(x + \\alpha h) - f(x)] = \\delta f(x; h). \n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzqh_O9oBNIU"
      },
      "source": [
        ">*Note:* By the [Encylopedia of Mathematics](https://encyclopediaofmath.org/index.php?title=Gâteaux_variation), it is stated that the **Gateaux differential** need not necessarily be a linear functional $h$. If for some fixed $x$, $\n",
        "\\delta f(x; h)$ is linear with respect to $h$, then\n",
        "$$\n",
        "\\delta f(x; h) = f'(x) h,\n",
        "$$\n",
        "where the operator $f'(x) \\in L(X, Y)$ is called the **Gateaux derivative** at x. The function $f'$ maps $D \\subseteq X$ to $L(X, Y)$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s_s6zx3Fo8s"
      },
      "source": [
        "### Examples regarding Gateaux differential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SalyPLcEA9b3"
      },
      "source": [
        "> **Example**: Find the Gateaux differential of\n",
        "$$F = \n",
        "\\begin{cases}\n",
        "\\frac{x^3}{x^2 + y^2} &\\text{ if } (x, y) \\neq 0;\\\\\n",
        "0 &\\text{ if } (x, y) = 0\n",
        "\\end{cases}\n",
        "$$\n",
        "at $(x, y) = (0, 0)$ and at $(x, y) \\neq (0, 0)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdSisDo-dqnc"
      },
      "source": [
        "**Example 1:** Let $X = E^n$ and $f(x) = f(x_1, x_2, \\ldots, x_n)$ be a functional on $E^n$ having continuous partial derivatives with respect to each $x_i$. Show that\n",
        "$$\n",
        "\\delta f(x; h) = \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i} h_i.\n",
        "$$\n",
        "\n",
        "**Proof:** First, we need to show that $f$ is Gateaux differentiable. By assumption, we know that $f$ has continuous partial derivatives with respect to each $x_i$, meaning that $f$ is continuously differentiable. It is also worth noting that if a function $\\mathbb{R}^n \\to \\mathbb{R}$ is continuously differentiable, then it is (Frechet-, which we will later cover, and therefore, Gateaux-) differentiable. See page 47-49 of this [link](http://www.supermath.info/AdvancedCalculus2017.pdf)) for more details. \n",
        "\n",
        "\n",
        "Then, by the chain rule, the Gateaux differential of $f$ is\n",
        "\\begin{align}\n",
        "\\delta f(x;h) &= \\frac{d}{d\\alpha} f(x + \\alpha h) \\bigg\\lvert_{\\alpha = 0} = \\frac{d}{d\\alpha} f(x_1 + \\alpha h_1, x_2 + \\alpha h_2, \\ldots, x_n + \\alpha h_2) \\bigg\\lvert_{\\alpha = 0}\\\\\n",
        "& = \\sum_{i=1}^n \\left(\\frac{\\partial f(x + \\alpha h)}{\\partial (x_i + \\alpha h_i)} \\frac{d (x_i + \\alpha h_i)}{d \\alpha}\\right) \\bigg\\lvert_{\\alpha = 0}\\\\\n",
        "& = \\sum_{i=1}^n \\frac{\\partial f(x + \\alpha h)}{\\partial (x_i + \\alpha h_i)}\\bigg\\lvert_{\\alpha = 0}  h_i = \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i} h_i.\n",
        "\\end{align}\n",
        "The last line is true because $\\frac{\\partial f(x + \\alpha h)}{\\partial (x_i + \\alpha h_i)}$ and $\\frac{\\partial f(x)}{\\partial x_i}$ are equal, since they have the same \"form\" of input, and the expression $\\alpha = 0$ eventually became irrelevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhlZn07mQZdb"
      },
      "source": [
        "**Example 2**: Let $X=C[0,1]$ and let $f(x)=\\int_0^1g(x(t),t)dt$, where it is assumed that the function $g_x$ exists and is continuous with respect to $x$ and $t$. Then\n",
        "$$\n",
        "\\delta f(x; h) = \\int_0^1g_x(x,t)h(t)dt\n",
        "$$\n",
        "**Proof**: Using the definition of Gateaux differential:\n",
        "\n",
        "\\begin{align}\n",
        "\\delta f(x; h) &= \\frac {d} {d\\alpha} \\int_0^1g(x(t)+\\alpha h(t),t)dt\\bigg\\lvert_{\\alpha = 0}\\\\\n",
        "&=\\int_0^1\\frac {d} {d\\alpha}g(x(t)+\\alpha h(t),t)dt\\bigg\\lvert_{\\alpha = 0}\\\\\n",
        "&=\\int_0^1(\\frac {\\partial g(x+\\alpha h,t)} {\\partial (x+\\alpha h)} \\frac {d(x+\\alpha h)} {d\\alpha}+\\frac {\\partial g(x+\\alpha h,t)} {\\partial t} \\frac {dt} {d\\alpha})dt\\bigg\\lvert_{\\alpha = 0}\\\\\n",
        "&=\\int_0^1\\frac {\\partial g(x,t)} {\\partial x} h(t)dt=\\int_0^1g_x(x,t)h(t)dt.\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To_bdpKdQZDX"
      },
      "source": [
        "**Example 3:** If $X = E^n$ and $Y = E^m$, and $T$ is a continuously differentiable mapping of $X$ into $Y$, then $\\delta T(x; h)$ exists and is equal to\n",
        "$$\n",
        "\\delta T(x; h) = Ah,\n",
        "$$\n",
        "where\n",
        "$$\n",
        "A = \n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial T_1}{\\partial x_1} & \\frac{\\partial T_1}{\\partial x_2} & \\cdots &\\frac{\\partial T_1}{\\partial x_n} \\\\\n",
        "\\frac{\\partial T_2}{\\partial x_1} & \\frac{\\partial T_2}{\\partial x_2}  & & \\frac{\\partial T_2}{\\partial x_n} \\\\\n",
        "\\vdots &  & \\ddots &\\vdots\\\\ \n",
        "\\frac{\\partial T_m}{\\partial x_1} & \\frac{\\partial T_m}{\\partial x_2} & \\cdots & \\frac{\\partial T_m}{\\partial x_n}\\\\\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "**Proof**: Since we know that $T$ is continuously differentiable, the transformation $T$ Gateaux differentiable. But what does it mean to be **continuously differentiable**? Continuity of the derivative of $T$ means that the mapping $T': x \\mapsto T'(x)$ is continuous, where $T'(x): D \\subseteq X \\to Y$ is an operator; note that the distance defined range of $T'$ is the operator norm. It is also worth noting that if a function $\\mathbb{R}^n \\to \\mathbb{R}^m$ is continuously differentiable, then it is (Frechet-, which we will later cover and therefore, Gateaux-) differentiable. See page 47-49 of this [link](http://www.supermath.info/AdvancedCalculus2017.pdf)) for more details. \n",
        "\n",
        "\n",
        "We can also rewrite $T$ as $T(x) = (T_1(x), T_2(x), \\ldots, T_m(x))^T$, where $T_i: E^n \\to E$. See the theorem about component functions on page 15 of this [link](http://www.supermath.info/AdvancedCalculus2017.pdf) for more details. Thus, \n",
        "\n",
        "\\begin{align}\n",
        "\\delta T(x;h) &= \\frac{d}{d\\alpha} T(x + \\alpha h) \\bigg\\lvert_{\\alpha = 0} = \\left(\\frac{d}{d\\alpha} T_1(x + \\alpha h), \\frac{d}{d\\alpha} T_2(x + \\alpha h), \\ldots, \\frac{d}{d\\alpha} T_m(x + \\alpha h)  \\right)^T  \\bigg\\lvert_{\\alpha = 0}\n",
        "\\end{align}\n",
        "\n",
        "By Example 1, we know that\n",
        "$$\n",
        "\\frac{d}{d\\alpha} T_i(x + \\alpha h) \\bigg\\lvert_{\\alpha = 0} = \\sum_{j = 1}^n \\frac{\\partial T_i}{\\partial x_j} h_j = \\left(\\frac{\\partial T_i}{\\partial x_1}, \\frac{\\partial T_i}{\\partial x_2}, \\ldots, \\frac{\\partial T_i}{\\partial x_n}\\right) h.\n",
        "$$\n",
        "\n",
        "Thus, we can compress the expression in the form $\\delta T(x;h) = A h$ as above. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_yWLienCbry"
      },
      "source": [
        "### Frechet Differential\n",
        "\n",
        "**Definition:** Let $T$ be a transformation defined on an open domain $D$ in a normed space $(X, \\lVert\\cdot\\rVert_X)$ having range in a normed space $(Y, \\lVert\\cdot\\rVert_Y)$. If for fixed $x \\in D$ and each $h \\in X$ there exists $\\delta T(x; h) \\in Y$ which is linear and continuous with respect to $h$ such that\n",
        "$$\n",
        "\\lim_{\\lVert h \\rVert \\to 0} \\frac{\\lVert T(x + h) - T(x) - \\delta T(x; h) \\rVert_X}{\\lVert h \\rVert_Y} = 0,\n",
        "$$\n",
        "then $T$ is said to be **Frechet differentiable** at $x$ and $\\delta T(x; h)$ is said to be the **Frechet differential** of $T$ at $x$ with increment $h$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLhKFIzXZGQG"
      },
      "source": [
        ">*Note*: By definition, it is obvious that if $T$ is Frechet differentiable, then $T$ is Gateaux differentiable.\n",
        "\n",
        ">*Note*: If $T$ is linear and bounded then $\\delta T(x; h)$ is linear and continuous with respect to $h$ since:\n",
        "$$\n",
        "\\delta T(x; h) = \\lim_{\\alpha \\to 0} \\frac{1}{\\alpha} [T(x + \\alpha h) -  T(x)] = T(h).\n",
        "$$\n",
        "However, the Frechet differentiable does not require $T$ to be linear, but $T'(h) : = \\delta T(x; h)$ must be linear and bounded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q061oxWPQfEq"
      },
      "source": [
        ">*Note*: We want to see the parallel in \"form\" between the definition of a Frechet differentiable function and the definition of a differentiable single-variable function. We know a single variable function $f: \\mathbb{R} \\to \\mathbb{R}$ is differentiable at $x$ if the derivative $f'$ at $x$ exists, meaning that\n",
        "$$\n",
        "f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}.\n",
        "$$\n",
        "This means that there is a mapping $f': \\mathbb{R} \\to \\mathbb{R}$ defined at $x$ such that\n",
        "$$\n",
        "\\lim_{h \\to 0} \\frac{f(x+h) - f(x) - f'(x) h}{h} = 0.\n",
        "$$\n",
        "Note that $f'(x) h$ is the differential, and this simple multiplication could be seen as an operator $f'(x): \\mathbb{R} \\to \\mathbb{R}$ acting on $h$. \n",
        "\n",
        ">In general normed vector space, we need to get around the fact that there is no division of one vector to another (and also the division elements between different vector spaces). Thus, we need to define Frechet differentiability as:\n",
        "$$\n",
        "\\lim_{\\lVert h \\rVert \\to 0} \\frac{\\lVert T(x + h) - T(x) - \\delta T(x; h) \\rVert_X}{\\lVert h \\rVert_Y} = 0.\n",
        "$$ \n",
        "Because $\\delta T(x; h)$ is linear, we can rewrite it as $f'(x)h$, where $f'(x) \\in L(X, Y)$. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbddR5u4IqmX"
      },
      "source": [
        "### Relationships between the Gateaux and Frechet Differentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_dgykiUCb2c"
      },
      "source": [
        "**Proposition 1:** If the transformation $T$ has a Frechet differential, it is unique.\n",
        "\n",
        "**Proof**: Fix $x$ and $h$. Suppose both $\\delta T(x;h)$ and $\\delta 'T(x;h)$ satisfy the requirements of the last definition. Since\n",
        "$$\n",
        "\\lVert \\delta T(x;h) - \\delta 'T(x;h) \\rVert = \\lVert - [T(x+h)-T(x) - \\delta T(x;h)] + [T(x+h)-T(x) - \\delta 'T(x;h)] \\rVert,\n",
        "$$\n",
        "Then by the traingle inequality\n",
        "$$\n",
        "\\lVert \\delta T(x;h) - \\delta 'T(x;h) \\rVert \\leq \\lVert T(x+h)-T(x)-\\delta T(x;h) \\rVert + \\lVert T(x+h)-T(x)-\\delta 'T(x;h) \\rVert\n",
        "$$\n",
        "or $\\lVert \\delta T(x;h) - \\delta 'T(x;h) \\rVert=o(\\lVert h \\rVert)$. Since $\\delta T(x;h)-\\delta 'T(x;h)$ is bounded and linear in $h$, it must be zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWo2t61U7tpk"
      },
      "source": [
        "Because $\\delta T(x;h)-\\delta 'T(x;h)$ is linear in $h$, we have\n",
        "\\begin{align}\n",
        "\\lvert \\alpha \\rvert \\lVert \\delta T(x;  h) - \\delta 'T(x; h) \\rVert &= \\lVert \\delta T(x; \\alpha h) - \\delta 'T(x;\\alpha h) \\rVert\\\\ \n",
        "& \\leq \\lVert T(x+\\alpha h)-T(x)-\\delta T(x;\\alpha h) \\rVert + \\lVert T(x+ \\alpha h)-T(x)-\\delta 'T(x;\\alpha h) \\rVert.\n",
        "\\end{align}\n",
        "\n",
        "Because $\\delta T(x;h)-\\delta 'T(x;h)$ is bounded in $h$, there exists a smallest $M \\geq 0$ such that $\\lVert \\delta T(x;h)-\\delta 'T(x;h) \\rVert \\leq M \\lVert h \\rVert$. Suppose that $M >0$.\n",
        "\n",
        "\n",
        "Because $\\delta T(x;k)$ and $\\delta 'T(x;k)$ are both Frechet differentials, it is true that  \n",
        "$$\n",
        "\\lim_{\\lVert k \\rVert \\to 0} \\frac{\\lVert T(x + k) - T(x) - \\delta T(x; k) \\rVert}{\\lVert k \\rVert} = 0 \\text{ and } \\lim_{\\lVert k \\rVert \\to 0} \\frac{\\lVert T(x + k) - T(x) - \\delta' T(x; k) \\rVert}{\\lVert k \\rVert} = 0.\n",
        "$$\n",
        "Hence, for all $\\varepsilon > 0$, there exists $\\delta>0$ such that for all $\\lVert k \\rVert < \\delta$, it must be true that \n",
        "$$\n",
        "\\lVert T(x + k) - T(x) - \\delta T(x; k) \\rVert < \\varepsilon \\lVert k \\rVert \\text{ and } \\lVert T(x + k) - T(x) - \\delta' T(x; k) \\rVert < \\varepsilon \\lVert k \\rVert.\n",
        "$$\n",
        "Choose $\\varepsilon < M/2$ and choose $\\alpha$ to be small so that $\\lVert \\alpha h \\rVert \\leq \\delta$. Thus, \n",
        "\\begin{align}\n",
        "\\lvert\\alpha\\rvert \\lVert \\delta T(x;  h) - \\delta 'T(x; h) \\rVert & \\leq \\lVert T(x+\\alpha h)-T(x)-\\delta T(x;\\alpha h) \\rVert + \\lVert T(x+ \\alpha h)-T(x)-\\delta 'T(x;\\alpha h) \\rVert\\\\\n",
        "&\\leq \\varepsilon \\lVert \\alpha h \\rVert + \\varepsilon \\lVert \\alpha h \\rVert\\\\\n",
        "&= 2\\varepsilon \\lvert\\alpha\\rvert \\lVert  h \\rVert.\\\\\n",
        "\\end{align}\n",
        "Thus, $\\lVert \\delta T(x;  h) - \\delta 'T(x; h) \\rVert \\leq 2\\varepsilon \\lVert  h \\rVert < M \\lVert  h \\rVert$. This is contradictory, since $M$ is already the smallest costant. Hence, $M = 0$ meaning that $\\delta T(x;h) = \\delta 'T(x;h)$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uef-9wwCbx3"
      },
      "source": [
        "**Proposition 2.** If the Frechet differential of $T$ exists at $x$, then the Gateaux differential exists at $x$ and they are equal.\n",
        "\n",
        "**Proof**: Denote Frechet differential by $\\delta T(x;h)$. By definition we have for any $h$,\n",
        "$$\n",
        "\\lim_{\\alpha \\to 0} \\frac{\\lVert T(x + \\alpha h) - T(x) - \\delta T(x; \\alpha h) \\rVert}{\\alpha} = 0\n",
        "$$ \n",
        "Since norm is continuous, one can have:\n",
        "$$\n",
        "\\lim_{\\alpha \\to 0} \\frac{\\lVert T(x + \\alpha h) - T(x) - \\delta T(x; \\alpha h) \\rVert}{\\alpha} = \\left\\lVert \\lim_{\\alpha \\to 0} \\frac{ T(x + \\alpha h) - T(x) - \\delta T(x; \\alpha h)} {\\alpha} \\right\\rVert = 0\n",
        "$$\n",
        "That means:\n",
        "$$\n",
        "\\lim_{\\alpha \\to 0} \\frac{ T(x + \\alpha h) - T(x) - \\delta T(x; \\alpha h)} {\\alpha}=0\n",
        "$$\n",
        "Since $\\delta T(x;h)$ is linear with respect to $h$, one has:\n",
        "\\begin{align}\n",
        "\\lim_{\\alpha \\to 0} \\frac{ T(x + \\alpha h) - T(x) } {\\alpha}&=\\lim_{\\alpha \\to 0} \\frac{\\delta T(x; \\alpha h)} {\\alpha}=\\lim_{\\alpha \\to 0} \\alpha\\frac{\\delta T(x; h)} {\\alpha}\\\\\n",
        "&=\\delta T(x; h)\n",
        "\\end{align}\n",
        "The LHS is, by definition, the Gateaux differential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTzrVEU9Cbzf"
      },
      "source": [
        "**Proposition 3.** If the transformation $T$ defined on an open set $D$ in $X$ has a Frechet differential at $x$, then $T$ is continuous at $x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAFP3kB5Cbud"
      },
      "source": [
        "**Proof:** Given $\\varepsilon >0$, there is a sphere about $x$ such that for $x + h$ in this sphere\n",
        "$$\n",
        "\\lVert T(x + h) - T(x) - \\delta T(x; h)\\rVert < \\varepsilon \\lVert h\\rVert.\n",
        "$$\n",
        "By the triangle inequality, we have\n",
        "$$\n",
        "\\lVert T(x + h) - T(x) \\rVert - \\lVert\\delta T(x; h) \\rVert \\leq \\lVert T(x + h) - T(x) - \\delta T(x; h)\\rVert < \\varepsilon \\lVert h\\rVert.\n",
        "$$\n",
        "Thus $\\lVert T(x + h) - T(x) \\rVert < \\varepsilon \\lVert h\\rVert + \\lVert\\delta T(x; h) \\rVert < M\\lVert h\\rVert.$ Since the transformation $T$ is bounded, it is continuous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT93vYWikyRI"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGuOZAt2Cbo8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJUXapZbCbhm"
      },
      "source": [
        "**Example 4**: From Example 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftfMPpXBCbXz"
      },
      "source": [
        "> *Note*: The differences between differential and derivative: the derivative of a function is the rate of change of the output value with respect to its input value; whereas differential is the actual change of function. For example if $y = f(x)$, then the differential of $y$ is $dy = f'(x)dx$, whereas the derivative of $y$ with respect to $x$ is $dy/dx = f'(x)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zftbMyb-RFN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}